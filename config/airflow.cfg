[core]
# Dossier racine pour Airflow
airflow_home = /opt/airflow

# Backend de base de données (PostgreSQL dans votre cas)
sql_alchemy_conn = postgresql+psycopg2://postgres:password@postgres:5432/airflow

# Chargement du DAG
executor = CeleryExecutor
dags_folder = /opt/airflow/dags
load_examples = False
default_timezone = utc

# Logs
remote_logging = False
base_log_folder = /opt/airflow/logs
remote_base_log_folder = /opt/airflow/logs
remote_log_conn_id =

[logging]
# Répertoires de logs
base_log_folder = /opt/airflow/logs
scheduler_log_folder = /opt/airflow/logs/scheduler
log_level = INFO
logging_config_class =

[scheduler]
# Réglages du planificateur (scheduler)
catchup_by_default = False
max_active_runs_per_dag = 1
scheduler_heartbeat_sec = 5
min_file_process_interval = 30

[webserver]
# Réglages du serveur Web
web_server_port = 8080
web_server_host = 0.0.0.0
workers = 4
expose_config = True
rbac = True

[celery]
# Config Celery pour l'exécution distribuée
broker_url = redis://redis:6379/0
result_backend = db+postgresql://postgres:password@postgres:5432/airflow
flower_host = 0.0.0.0
flower_port = 5555
worker_concurrency = 4

[database]
# Base de données utilisée par Airflow
sql_alchemy_conn = postgresql+psycopg2://postgres:password@postgres:5432/airflow
